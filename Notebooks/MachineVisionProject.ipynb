{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Machine Vision Project**\n",
        "### **Object detection in radar images (ship detection)**\n",
        "\n",
        "### **Project objective:**\n",
        "\n",
        "The main intention behind this project is to develop a tool capable of identifying ships from radar imagery, either in static images or video sequences. The objective is to design and evaluate an object detection pipeline that can automatically locate and classify ships within complex radar data, where noise, low contrast, and environmental interference are common challenges.\n",
        "\n",
        "This goal is achieved through the development of a recognition system implemented in Python, using a Jupyter Notebook as the main experimentation and documentation environment. The notebook format allows for an iterative workflow where data preprocessing, model training, evaluation, and visualization coexist in a single, transparent space.\n",
        "\n",
        "### **Motivation**\n",
        "\n",
        "Maritime surveillance is a critical task in areas such as port security, border control, illegal fishing detection, and collision avoidance systems. Radar imagery plays a key role in these scenarios due to its robustness under adverse weather conditions and low-visibility environments, where traditional optical sensors often fail. However, interpreting radar images manually is time-consuming and prone to human error.\n",
        "\n",
        "By applying machine vision techniques to radar data, this project explores how automated systems can assist or replace manual monitoring, improving efficiency, scalability, and response time. In short: less eyeballing blips, more intelligence.\n",
        "\n",
        "### **Approach Overview**\n",
        "\n",
        "The project follows a classical machine vision pipeline adapted to radar imagery:\n",
        "\n",
        "- Data exploration and preprocessing to reduce noise and enhance relevant features\n",
        "\n",
        "- Annotation and preparation of training data\n",
        "\n",
        "- Application of object detection techniques (traditional or learning-based, depending on constraints)\n",
        "\n",
        "- Model evaluation using appropriate metrics to assess detection accuracy and robustness\n",
        "\n",
        "Special attention is given to the unique characteristics of radar images, such as speckle noise, varying signal intensity, and ambiguous object boundaries.\n",
        "\n",
        "### **Scope and Limitations**\n",
        "\n",
        "The scope of this project is limited to ship detection within the provided dataset and does not aim to perform ship tracking or identification beyond basic object detection. Additionally, performance may vary depending on radar resolution, environmental conditions, and dataset diversity. These limitations are acknowledged and discussed as part of the final evaluation.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QD5-z6SaEqxm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### First quick checks\n",
        "\n",
        "These checks are only for quick verifications so everything runs as smoothly as possible in the notebook and the execution enviroment, which in this case it is Google Collab."
      ],
      "metadata": {
        "id": "G6CnFeYvKXNn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xj5wKIsBEW6O",
        "outputId": "ff04697f-1897-4e12-9eb9-59db2c9f2c74",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
            "Platform: Linux-6.6.105+-x86_64-with-glibc2.35\n",
            "Torch: 2.9.0+cpu\n",
            "CUDA available: False\n"
          ]
        }
      ],
      "source": [
        "# (Recommended) Run on a GPU runtime\n",
        "import os, sys, platform\n",
        "print('Python:', sys.version)\n",
        "print('Platform:', platform.platform())\n",
        "\n",
        "# Optional: quick CUDA check (PyTorch may not be installed yet)\n",
        "try:\n",
        "    import torch\n",
        "    print('Torch:', torch.__version__)\n",
        "    print('CUDA available:', torch.cuda.is_available())\n",
        "    if torch.cuda.is_available():\n",
        "        print('GPU:', torch.cuda.get_device_name(0))\n",
        "except Exception as e:\n",
        "    print('Torch not available yet (will install below).')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing necessary dependencies"
      ],
      "metadata": {
        "id": "7Ot2LxMdL1HR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "# Ultralytics provides YOLO training/inference; pycocotools reads COCO JSON; shapely helps geometry.\n",
        "\n",
        "!pip -q install --upgrade ultralytics\n",
        "!pip -q install pycocotools shapely opencv-python matplotlib tqdm pyyaml\n",
        "\n",
        "# Restart runtime if Ultralytics asks you to.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jen7WLNqLMBo",
        "outputId": "84d873d6-63ea-4647-c2d3-3abdbabef256"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) Get the HRSID dataset\n",
        "\n",
        "HRSID images are large; downloading may take a while.\n",
        "\n",
        "You have two options:\n",
        "\n",
        "### Option A — Manual download (recommended)\n",
        "1. Download HRSID (JPG) from the links in the official repository.\n",
        "2. Extract it into a folder like:\n",
        "\n",
        "```\n",
        "./HRSID_ROOT/\n",
        "  images/...\n",
        "  annotations/...\n",
        "```\n",
        "\n",
        "### Option B — Programmatic download (Google Drive)\n",
        "If the dataset is hosted on Google Drive, you can often download it with `gdown` using the file id.\n",
        "\n",
        "> If your download fails (quota, permissions), use manual download."
      ],
      "metadata": {
        "id": "WPWqwz9kMakU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Point this to where you placed/extracted the dataset\n",
        "HRSID_ROOT = Path('HRSID_ROOT')  # <-- change me\n",
        "\n",
        "# Expected subfolders (adjust if your extraction differs)\n",
        "ANNOT_DIR = HRSID_ROOT / 'annotations'\n",
        "IMG_DIR = HRSID_ROOT / 'images'\n",
        "\n",
        "print('HRSID_ROOT:', HRSID_ROOT.resolve())\n",
        "print('ANNOT_DIR exists:', ANNOT_DIR.exists())\n",
        "print('IMG_DIR exists:', IMG_DIR.exists())\n",
        "\n",
        "# List files (helps you discover exact structure)\n",
        "if ANNOT_DIR.exists():\n",
        "    print('Annotation files:', sorted([p.name for p in ANNOT_DIR.glob('*.json')])[:10])\n",
        "if IMG_DIR.exists():\n",
        "    exts = ['*.jpg','*.jpeg','*.png']\n",
        "    imgs = []\n",
        "    for e in exts:\n",
        "        imgs += list(IMG_DIR.rglob(e))\n",
        "    print('Found images:', len(imgs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNn_37SGMiah",
        "outputId": "02b96421-85b6-4f1d-ff11-48c75aed4af9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HRSID_ROOT: /content/HRSID_ROOT\n",
            "ANNOT_DIR exists: False\n",
            "IMG_DIR exists: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Set these based on what exists in your annotations folder.\n",
        "COCO_TRAIN_JSON = None\n",
        "COCO_VAL_JSON = None\n",
        "COCO_TEST_JSON = None\n",
        "\n",
        "# Auto-detect common names\n",
        "cands = list((ANNOT_DIR).glob('*.json')) if ANNOT_DIR.exists() else []\n",
        "name2path = {p.name.lower(): p for p in cands}\n",
        "\n",
        "for key in ['train.json','train2017.json','hrsid_train.json','instances_train.json']:\n",
        "    if key in name2path:\n",
        "        COCO_TRAIN_JSON = name2path[key]\n",
        "        break\n",
        "\n",
        "for key in ['val.json','valid.json','hrsid_val.json','instances_val.json','val2017.json']:\n",
        "    if key in name2path:\n",
        "        COCO_VAL_JSON = name2path[key]\n",
        "        break\n",
        "\n",
        "for key in ['test.json','hrsid_test.json','instances_test.json']:\n",
        "    if key in name2path:\n",
        "        COCO_TEST_JSON = name2path[key]\n",
        "        break\n",
        "\n",
        "# If no train JSON found, fall back to the first json in the folder\n",
        "if COCO_TRAIN_JSON is None and cands:\n",
        "    COCO_TRAIN_JSON = cands[0]\n",
        "\n",
        "print('COCO_TRAIN_JSON:', COCO_TRAIN_JSON)\n",
        "print('COCO_VAL_JSON:', COCO_VAL_JSON)\n",
        "print('COCO_TEST_JSON:', COCO_TEST_JSON)"
      ],
      "metadata": {
        "id": "H1uGJgGtM7_N",
        "outputId": "fa4eddae-36c3-4a29-c79d-7ef17b6ee84d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "COCO_TRAIN_JSON: None\n",
            "COCO_VAL_JSON: None\n",
            "COCO_TEST_JSON: None\n"
          ]
        }
      ]
    }
  ]
}