{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e6e485e",
   "metadata": {},
   "source": [
    "# Ship Detection in SAR Images with YOLO-OBB (HRSID)\n",
    "\n",
    "**Task:** Object detection (ship detection) in Synthetic Aperture Radar (SAR) images.\n",
    "\n",
    "**Dataset:** HRSID (High Resolution SAR Images Dataset) — COCO-style annotations with rotated ship instances.\n",
    "\n",
    "**Model:** Ultralytics YOLO **OBB** (oriented bounding boxes).\n",
    "\n",
    "---\n",
    "\n",
    "## Notebook roadmap\n",
    "1. Environment setup\n",
    "2. Download / place the HRSID dataset\n",
    "3. Parse COCO annotations and convert to **YOLO OBB label format** (`class x1 y1 ... x4 y4`, normalized)\n",
    "4. Visual sanity checks (overlay boxes)\n",
    "5. Train YOLO-OBB\n",
    "6. Evaluate and run inference\n",
    "\n",
    "> Notes\n",
    "> - This notebook focuses on a clean, reproducible pipeline.\n",
    "> - Training needs a GPU for reasonable speed. CPU training works but is slow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac80934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Recommended) Run on a GPU runtime\n",
    "import os, sys, platform\n",
    "print('Python:', sys.version)\n",
    "print('Platform:', platform.platform())\n",
    "\n",
    "# Optional: quick CUDA check (PyTorch may not be installed yet)\n",
    "try:\n",
    "    import torch\n",
    "    print('Torch:', torch.__version__)\n",
    "    print('CUDA available:', torch.cuda.is_available())\n",
    "    if torch.cuda.is_available():\n",
    "        print('GPU:', torch.cuda.get_device_name(0))\n",
    "except Exception as e:\n",
    "    print('Torch not available yet (will install below).')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c670b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "# Ultralytics provides YOLO training/inference; pycocotools reads COCO JSON; shapely helps geometry.\n",
    "\n",
    "!pip -q install --upgrade ultralytics\n",
    "!pip -q install pycocotools shapely opencv-python matplotlib tqdm pyyaml\n",
    "\n",
    "# Restart runtime if Ultralytics asks you to.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee3c416",
   "metadata": {},
   "source": [
    "## 1) Get the HRSID dataset\n",
    "\n",
    "HRSID images are large; downloading may take a while.\n",
    "\n",
    "You have two options:\n",
    "\n",
    "### Option A — Manual download (recommended)\n",
    "1. Download HRSID (JPG) from the links in the official repository.\n",
    "2. Extract it into a folder like:\n",
    "\n",
    "```\n",
    "./HRSID_ROOT/\n",
    "  images/...\n",
    "  annotations/...\n",
    "```\n",
    "\n",
    "### Option B — Programmatic download (Google Drive)\n",
    "If the dataset is hosted on Google Drive, you can often download it with `gdown` using the file id.\n",
    "\n",
    "> If your download fails (quota, permissions), use manual download.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f905ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Point this to where you placed/extracted the dataset\n",
    "HRSID_ROOT = Path('HRSID_ROOT')  # <-- change me\n",
    "\n",
    "# Expected subfolders (adjust if your extraction differs)\n",
    "ANNOT_DIR = HRSID_ROOT / 'annotations'\n",
    "IMG_DIR = HRSID_ROOT / 'images'\n",
    "\n",
    "print('HRSID_ROOT:', HRSID_ROOT.resolve())\n",
    "print('ANNOT_DIR exists:', ANNOT_DIR.exists())\n",
    "print('IMG_DIR exists:', IMG_DIR.exists())\n",
    "\n",
    "# List files (helps you discover exact structure)\n",
    "if ANNOT_DIR.exists():\n",
    "    print('Annotation files:', sorted([p.name for p in ANNOT_DIR.glob('*.json')])[:10])\n",
    "if IMG_DIR.exists():\n",
    "    exts = ['*.jpg','*.jpeg','*.png']\n",
    "    imgs = []\n",
    "    for e in exts:\n",
    "        imgs += list(IMG_DIR.rglob(e))\n",
    "    print('Found images:', len(imgs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38fe693",
   "metadata": {},
   "source": [
    "## 2) Choose the COCO annotation JSON(s)\n",
    "\n",
    "HRSID uses COCO-like JSON. Some releases include separate files for train/val/test; others provide a single JSON.\n",
    "\n",
    "- If you have `train.json` and `test.json`, set them below.\n",
    "- If you only have one JSON, we will create a train/val split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d39de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Set these based on what exists in your annotations folder.\n",
    "COCO_TRAIN_JSON = None\n",
    "COCO_VAL_JSON = None\n",
    "COCO_TEST_JSON = None\n",
    "\n",
    "# Auto-detect common names\n",
    "cands = list((ANNOT_DIR).glob('*.json')) if ANNOT_DIR.exists() else []\n",
    "name2path = {p.name.lower(): p for p in cands}\n",
    "\n",
    "for key in ['train.json','train2017.json','hrsid_train.json','instances_train.json']:\n",
    "    if key in name2path:\n",
    "        COCO_TRAIN_JSON = name2path[key]\n",
    "        break\n",
    "\n",
    "for key in ['val.json','valid.json','hrsid_val.json','instances_val.json','val2017.json']:\n",
    "    if key in name2path:\n",
    "        COCO_VAL_JSON = name2path[key]\n",
    "        break\n",
    "\n",
    "for key in ['test.json','hrsid_test.json','instances_test.json']:\n",
    "    if key in name2path:\n",
    "        COCO_TEST_JSON = name2path[key]\n",
    "        break\n",
    "\n",
    "# If no train JSON found, fall back to the first json in the folder\n",
    "if COCO_TRAIN_JSON is None and cands:\n",
    "    COCO_TRAIN_JSON = cands[0]\n",
    "\n",
    "print('COCO_TRAIN_JSON:', COCO_TRAIN_JSON)\n",
    "print('COCO_VAL_JSON:', COCO_VAL_JSON)\n",
    "print('COCO_TEST_JSON:', COCO_TEST_JSON)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95e2dbf",
   "metadata": {},
   "source": [
    "## 3) Convert COCO rotated instances to Ultralytics YOLO-OBB labels\n",
    "\n",
    "Ultralytics YOLO OBB expects one `*.txt` label per image:\n",
    "\n",
    "```\n",
    "class_index x1 y1 x2 y2 x3 y3 x4 y4\n",
    "```\n",
    "\n",
    "All coordinates are **normalized to [0, 1]** (divide by image width/height).\n",
    "\n",
    "HRSID COCO annotations typically store ship outlines as polygons in `segmentation`.\n",
    "We will:\n",
    "- read each annotation polygon\n",
    "- reduce it to 4 corners if it is already a rotated rectangle (8 numbers)\n",
    "- otherwise compute a minimum-area rectangle (requires shapely)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed6844e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "from shapely.geometry import Polygon\n",
    "from tqdm import tqdm\n",
    "\n",
    "RNG_SEED = 42\n",
    "random.seed(RNG_SEED)\n",
    "np.random.seed(RNG_SEED)\n",
    "\n",
    "# Output dataset in Ultralytics format\n",
    "OUT_ROOT = Path('hrsid_yolo_obb')\n",
    "OUT_IMAGES = OUT_ROOT / 'images'\n",
    "OUT_LABELS = OUT_ROOT / 'labels'\n",
    "\n",
    "(OUT_IMAGES / 'train').mkdir(parents=True, exist_ok=True)\n",
    "(OUT_IMAGES / 'val').mkdir(parents=True, exist_ok=True)\n",
    "(OUT_LABELS / 'train').mkdir(parents=True, exist_ok=True)\n",
    "(OUT_LABELS / 'val').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Single-class dataset: ship -> class 0\n",
    "CLASS_NAME = 'ship'\n",
    "CLASS_ID = 0\n",
    "\n",
    "\n",
    "def load_coco_json(path: Path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def coco_images_by_id(coco):\n",
    "    return {img['id']: img for img in coco.get('images', [])}\n",
    "\n",
    "\n",
    "def group_annotations_by_image(coco):\n",
    "    grouped = defaultdict(list)\n",
    "    for ann in coco.get('annotations', []):\n",
    "        grouped[ann['image_id']].append(ann)\n",
    "    return grouped\n",
    "\n",
    "\n",
    "def polygon_from_segmentation(seg):\n",
    "    # COCO segmentation can be list-of-lists or a flat list.\n",
    "    # We only handle polygon segmentations here.\n",
    "    if seg is None:\n",
    "        return None\n",
    "    if isinstance(seg, list) and len(seg) == 0:\n",
    "        return None\n",
    "    if isinstance(seg, list) and isinstance(seg[0], list):\n",
    "        # take the largest polygon part\n",
    "        seg = max(seg, key=lambda s: len(s))\n",
    "    if not isinstance(seg, list):\n",
    "        return None\n",
    "    if len(seg) < 6:\n",
    "        return None\n",
    "    pts = np.array(seg, dtype=float).reshape(-1, 2)\n",
    "    return pts\n",
    "\n",
    "\n",
    "def min_area_rect_corners(pts_xy):\n",
    "    # Use shapely's minimum_rotated_rectangle\n",
    "    poly = Polygon(pts_xy)\n",
    "    if not poly.is_valid:\n",
    "        poly = poly.buffer(0)\n",
    "    if poly.is_empty:\n",
    "        return None\n",
    "    mrr = poly.minimum_rotated_rectangle\n",
    "    coords = np.array(mrr.exterior.coords)[:-1]  # 4 points\n",
    "    if coords.shape != (4, 2):\n",
    "        return None\n",
    "    return coords\n",
    "\n",
    "\n",
    "def corners_to_yolo_line(corners_xy, w, h, class_id=0):\n",
    "    # corners_xy: (4,2) in pixel coords\n",
    "    corners = corners_xy.copy().astype(float)\n",
    "    corners[:, 0] = np.clip(corners[:, 0] / w, 0, 1)\n",
    "    corners[:, 1] = np.clip(corners[:, 1] / h, 0, 1)\n",
    "    flat = corners.reshape(-1)\n",
    "    return str(class_id) + ' ' + ' '.join([f'{v:.6f}' for v in flat])\n",
    "\n",
    "\n",
    "def resolve_image_path(file_name: str):\n",
    "    # HRSID may store relative paths. Try a few common patterns.\n",
    "    p = IMG_DIR / file_name\n",
    "    if p.exists():\n",
    "        return p\n",
    "    # search recursively as fallback (slower)\n",
    "    matches = list(IMG_DIR.rglob(Path(file_name).name))\n",
    "    return matches[0] if matches else None\n",
    "\n",
    "\n",
    "def convert_coco_to_yolo_obb(coco_json: Path, split_name: str, make_split=False, val_ratio=0.15):\n",
    "    coco = load_coco_json(coco_json)\n",
    "    imgs = coco.get('images', [])\n",
    "    img_by_id = coco_images_by_id(coco)\n",
    "    anns_by_img = group_annotations_by_image(coco)\n",
    "\n",
    "    # If this JSON should be split into train/val\n",
    "    if make_split:\n",
    "        ids = [im['id'] for im in imgs]\n",
    "        random.shuffle(ids)\n",
    "        n_val = int(len(ids) * val_ratio)\n",
    "        val_ids = set(ids[:n_val])\n",
    "        train_ids = set(ids[n_val:])\n",
    "        split_to_ids = {'train': train_ids, 'val': val_ids}\n",
    "    else:\n",
    "        split_to_ids = {split_name: set([im['id'] for im in imgs])}\n",
    "\n",
    "    stats = {'images': 0, 'instances': 0, 'skipped_instances': 0, 'skipped_images': 0}\n",
    "\n",
    "    for split, idset in split_to_ids.items():\n",
    "        for img_id in tqdm(sorted(idset), desc=f'Converting {split}'):\n",
    "            im = img_by_id.get(img_id)\n",
    "            if im is None:\n",
    "                continue\n",
    "\n",
    "            file_name = im.get('file_name')\n",
    "            width = im.get('width')\n",
    "            height = im.get('height')\n",
    "\n",
    "            # If width/height are missing, try reading the image\n",
    "            img_path = resolve_image_path(file_name)\n",
    "            if img_path is None:\n",
    "                stats['skipped_images'] += 1\n",
    "                continue\n",
    "\n",
    "            if width is None or height is None:\n",
    "                import cv2\n",
    "                arr = cv2.imread(str(img_path), cv2.IMREAD_GRAYSCALE)\n",
    "                if arr is None:\n",
    "                    stats['skipped_images'] += 1\n",
    "                    continue\n",
    "                height, width = arr.shape[:2]\n",
    "\n",
    "            lines = []\n",
    "            for ann in anns_by_img.get(img_id, []):\n",
    "                seg = ann.get('segmentation')\n",
    "                pts = polygon_from_segmentation(seg)\n",
    "                corners = None\n",
    "\n",
    "                # Fast path: segmentation is a rotated rectangle (4 points => 8 numbers)\n",
    "                if pts is not None and len(pts) == 4:\n",
    "                    corners = pts\n",
    "                else:\n",
    "                    # Compute minimum-area rectangle from polygon\n",
    "                    if pts is not None:\n",
    "                        corners = min_area_rect_corners(pts)\n",
    "\n",
    "                # Fallback: if no segmentation, try bbox (axis-aligned) and convert to corners\n",
    "                if corners is None:\n",
    "                    bbox = ann.get('bbox')  # [x,y,w,h] in COCO\n",
    "                    if bbox is not None and len(bbox) == 4:\n",
    "                        x, y, bw, bh = bbox\n",
    "                        corners = np.array([[x, y], [x+bw, y], [x+bw, y+bh], [x, y+bh]], dtype=float)\n",
    "\n",
    "                if corners is None:\n",
    "                    stats['skipped_instances'] += 1\n",
    "                    continue\n",
    "\n",
    "                lines.append(corners_to_yolo_line(corners, width, height, CLASS_ID))\n",
    "                stats['instances'] += 1\n",
    "\n",
    "            # Write label file (even if empty -> negative sample)\n",
    "            label_path = (OUT_LABELS / split / (Path(img_path).stem + '.txt'))\n",
    "            label_path.write_text('\n",
    "'.join(lines) + ('\n",
    "' if lines else ''), encoding='utf-8')\n",
    "\n",
    "            # Copy image\n",
    "            out_img_path = (OUT_IMAGES / split / Path(img_path).name)\n",
    "            if not out_img_path.exists():\n",
    "                out_img_path.write_bytes(img_path.read_bytes())\n",
    "\n",
    "            stats['images'] += 1\n",
    "\n",
    "    return stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5b88a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run conversion\n",
    "\n",
    "# Case 1: train/val JSONs already exist\n",
    "if COCO_TRAIN_JSON is not None and COCO_VAL_JSON is not None:\n",
    "    train_stats = convert_coco_to_yolo_obb(Path(COCO_TRAIN_JSON), split_name='train', make_split=False)\n",
    "    val_stats = convert_coco_to_yolo_obb(Path(COCO_VAL_JSON), split_name='val', make_split=False)\n",
    "    print('Train stats:', train_stats)\n",
    "    print('Val stats:', val_stats)\n",
    "\n",
    "# Case 2: only a single JSON -> create split inside it\n",
    "else:\n",
    "    assert COCO_TRAIN_JSON is not None, 'No COCO annotation JSON found.'\n",
    "    stats = convert_coco_to_yolo_obb(Path(COCO_TRAIN_JSON), split_name='train', make_split=True, val_ratio=0.15)\n",
    "    print('Split stats:', stats)\n",
    "\n",
    "print('Output root:', OUT_ROOT.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dffad2a",
   "metadata": {},
   "source": [
    "## 4) Visual sanity check: draw OBB labels on images\n",
    "\n",
    "We will sample a few converted images and overlay the 4-corner OBB polygons.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08845c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def read_yolo_obb_labels(label_path: Path):\n",
    "    lines = label_path.read_text(encoding='utf-8').strip().splitlines() if label_path.exists() else []\n",
    "    polys = []\n",
    "    for ln in lines:\n",
    "        if not ln.strip():\n",
    "            continue\n",
    "        parts = ln.strip().split()\n",
    "        cls = int(parts[0])\n",
    "        coords = np.array([float(x) for x in parts[1:]], dtype=float).reshape(4, 2)\n",
    "        polys.append((cls, coords))\n",
    "    return polys\n",
    "\n",
    "\n",
    "def denorm(coords_norm, w, h):\n",
    "    xy = coords_norm.copy()\n",
    "    xy[:, 0] *= w\n",
    "    xy[:, 1] *= h\n",
    "    return xy\n",
    "\n",
    "\n",
    "def show_samples(split='train', k=6):\n",
    "    img_paths = sorted((OUT_IMAGES / split).glob('*'))\n",
    "    if not img_paths:\n",
    "        print('No images found in', OUT_IMAGES / split)\n",
    "        return\n",
    "    picks = random.sample(img_paths, min(k, len(img_paths)))\n",
    "\n",
    "    for p in picks:\n",
    "        img = cv2.imread(str(p), cv2.IMREAD_GRAYSCALE)\n",
    "        if img is None:\n",
    "            continue\n",
    "        h, w = img.shape[:2]\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "        label_path = (OUT_LABELS / split / (p.stem + '.txt'))\n",
    "        polys = read_yolo_obb_labels(label_path)\n",
    "\n",
    "        for cls, poly_n in polys:\n",
    "            poly = denorm(poly_n, w, h).astype(int)\n",
    "            cv2.polylines(img_rgb, [poly.reshape(-1, 1, 2)], isClosed=True, color=(0, 255, 0), thickness=2)\n",
    "\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.title(f'{split}: {p.name} | ships: {len(polys)}')\n",
    "        plt.imshow(img_rgb)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "show_samples('train', k=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc130680",
   "metadata": {},
   "source": [
    "## 5) Create Ultralytics dataset YAML\n",
    "\n",
    "Ultralytics uses a YAML file that points to the dataset root and image folders.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75a4143",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "dataset_yaml = {\n",
    "    'path': str(OUT_ROOT),\n",
    "    'train': 'images/train',\n",
    "    'val': 'images/val',\n",
    "    'names': {0: CLASS_NAME},\n",
    "}\n",
    "\n",
    "yaml_path = OUT_ROOT / 'hrsid_obb.yaml'\n",
    "with open(yaml_path, 'w', encoding='utf-8') as f:\n",
    "    yaml.safe_dump(dataset_yaml, f, sort_keys=False)\n",
    "\n",
    "print('Wrote:', yaml_path)\n",
    "print(yaml_path.read_text())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab7e181",
   "metadata": {},
   "source": [
    "## 6) Train a YOLO-OBB model\n",
    "\n",
    "Choose a small model to start (fast iteration), then scale up.\n",
    "\n",
    "Ultralytics provides pretrained OBB models (e.g. `yolov8n-obb.pt` or newer OBB models).\n",
    "\n",
    "> Tip: If your ships are tiny, try increasing `imgsz` (e.g. 1024) and using multi-scale training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859fc3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Pick an OBB pretrained checkpoint.\n",
    "# If you get a \"file not found\" error, run `!yolo help` or check Ultralytics docs for the latest model names.\n",
    "model = YOLO('yolov8n-obb.pt')\n",
    "\n",
    "results = model.train(\n",
    "    data=str(yaml_path),\n",
    "    epochs=100,\n",
    "    imgsz=1024,\n",
    "    batch=8,\n",
    "    device=0,         # set to 'cpu' if no GPU\n",
    "    workers=4,\n",
    "    project='runs_hrsid',\n",
    "    name='yolov8n_obb_hrsid',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa80bf7",
   "metadata": {},
   "source": [
    "## 7) Evaluate (validation)\n",
    "\n",
    "Ultralytics will compute OBB detection metrics during training. You can also run validation explicitly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02afc67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the best checkpoint\n",
    "best = Path('runs_hrsid/yolov8n_obb_hrsid/weights/best.pt')\n",
    "model = YOLO(str(best) if best.exists() else 'yolov8n-obb.pt')\n",
    "\n",
    "metrics = model.val(data=str(yaml_path), imgsz=1024)\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdd4909",
   "metadata": {},
   "source": [
    "## 8) Inference + visualization\n",
    "\n",
    "We run inference on a few validation images and draw predicted OBBs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c679ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "val_imgs = sorted((OUT_IMAGES / 'val').glob('*'))\n",
    "if not val_imgs:\n",
    "    print('No val images found.')\n",
    "else:\n",
    "    sample_imgs = random.sample(val_imgs, min(6, len(val_imgs)))\n",
    "\n",
    "    # Use best checkpoint if present\n",
    "    best = Path('runs_hrsid/yolov8n_obb_hrsid/weights/best.pt')\n",
    "    model = YOLO(str(best) if best.exists() else 'yolov8n-obb.pt')\n",
    "\n",
    "    preds = model.predict(source=[str(p) for p in sample_imgs], imgsz=1024, conf=0.25)\n",
    "\n",
    "    for p, r in zip(sample_imgs, preds):\n",
    "        img = cv2.imread(str(p), cv2.IMREAD_GRAYSCALE)\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "        # r.obb.xyxyxyxy gives (N,4,2) in pixel coords when available\n",
    "        if getattr(r, 'obb', None) is not None and r.obb.xyxyxyxy is not None:\n",
    "            polys = r.obb.xyxyxyxy.cpu().numpy().astype(int)\n",
    "            for poly in polys:\n",
    "                cv2.polylines(img_rgb, [poly.reshape(-1,1,2)], True, (255,0,0), 2)\n",
    "\n",
    "        plt.figure(figsize=(6,6))\n",
    "        plt.title(f'Predictions: {p.name}')\n",
    "        plt.imshow(img_rgb)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efe7ea6",
   "metadata": {},
   "source": [
    "## 9) What to report (project write-up checklist)\n",
    "\n",
    "Include these in your final project report:\n",
    "\n",
    "- **Dataset**: size, train/val split, number of instances, typical ship sizes.\n",
    "- **Preprocessing**: conversion to YOLO-OBB, any filtering, handling of invalid polygons.\n",
    "- **Model**: YOLO OBB variant, image size, training epochs, augmentations.\n",
    "- **Metrics**: mAP@50, mAP@50-95 (OBB), PR curves if available.\n",
    "- **Qualitative results**: example detections (inshore vs offshore), failure cases.\n",
    "- **Ablations** (optional): imgsz 640 vs 1024; nano vs small model; conf threshold.\n",
    "\n",
    "---\n",
    "\n",
    "### Ideas for extensions (optional)\n",
    "- Train separate models for **inshore vs offshore** or add a domain label.\n",
    "- Add SAR-specific augmentations (speckle noise simulation, contrast stretching).\n",
    "- Use tiling (split large images into crops) for better small-object recall.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
